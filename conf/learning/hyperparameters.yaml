epochs: 1e2
optimizer: Adam
learning_rate: 1e-4
batch_size: 8

# Stopping criterions
early_stopping: False # By default no early stopping, so below are ignored
patience: 100 # how many consecutive epochs with no change
loss_threshold: 0.02
accuracy_threshold: 0.996

# Checkpoint saving
checkpoint_freq: 10 # Save the model checkpoint if it's the best so far or according to checkpoint_freq

init_temperature: 1e5 # initialization temp param for logits, larger -> more uniform

num_states: 10 # in general, depends on the quantifer, so should be overridden

threshold: 0.5 # greater than equal to this threshold for predicting True in  binary classification accuracy